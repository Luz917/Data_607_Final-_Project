---
title: "Data 607 Final Project"
author: "Maryluz Cruz"
date: "12/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data 607 Final Project

## Introduction and Motivation

#### For this assignment I will be scraping data from three different web articles that discuss how Social Media affects Adolescents and Children and how it affects there mental health. For this assignment I will do a text analysis of the scraped data and do a sentiment analysis based on the scraped text. The motivation behind this assignment is that I have a niece that is in her pre-teens sometimes I wonder if she got into Social Media too early. Based on these articles it seems like Social Media is not exactly a good thing for someone that is an Adolescents and Children.  

## First we load the packages
```{r warning=FALSE, message= FALSE}
require(rvest)
require(purrr)
require(xml2)  
require(rJava)
require(kableExtra)
require(dplyr)
require(tidytext)
require(tidyverse)
require(tm)
require(SnowballC)
require(ggplot2)
require(igraph)
require(ggraph)
require(widyr)
require(wordcloud)
require(tidyr)
require(reshape2)
```


## Load the Data 

### First article - Social Media and Adolescents’ and Young Adults’ Mental Health by Elina Mir and Caroline Novas, National Center for Health Research
- As the title states it discusses social media and adoloescents and mental health you can look at the story here[http://www.center4research.org/social-media-affects-mental-health/] 

- Read in the url for the first article 
```{r warning=FALSE, message= FALSE}
url <- "http://www.center4research.org/social-media-affects-mental-health/"  ## Firse Article
```



### Second Article- The Negative Effects of Social Media for Teens by Josh Ochs
- This this article talks about the negative affects of social media and you can look at the article here [https://smartsocial.com/negative-effects-of-social-media/]

- Read in the url for the second article 
```{r warning=FALSE, message= FALSE}
url1 <- "https://smartsocial.com/negative-effects-of-social-media/" ## Second Article 
```


### Third Article - Social Media and Kids: Some Benefits, Some Worries by The American Academy of Pediatrics
- This article basically discusses how social media has some benifits and of course some worries and you can look at the original article here [https://www.aap.org/en-us/about-the-aap/aap-press-room/pages/Social-Media-and-Kids-Some-Benefits,-Some-Worries.aspx]


_ Read in the third article 
```{r warning=FALSE, message= FALSE}
url2 <- "https://www.aap.org/en-us/about-the-aap/aap-press-room/pages/Social-Media-and-Kids-Some-Benefits,-Some-Worries.aspx"
```


#### Here we combine the first and second URL so we can scrape two of the articles at the same time 

```{r warning=FALSE, message= FALSE}
listofurls<-c(url,url1)
```


##### Using the rvest package we are able to scrape the two web articles. 
- We also convert the data collected into a data.frame. 
- SelectorGadget was used to find out what the html nodes are listed under. 

```{r warning=FALSE, message= FALSE}
article <- listofurls %>% map(read_html)
title <-
    article %>% map_chr(. %>% html_node("title") %>% html_text())
content <-
    article %>% map_chr(. %>% html_nodes('.entry-content')  %>%
                          html_text() %>% paste(., collapse = ""))
article_table <- data.frame("Title" = title, "Text" = content)
glimpse(article_table)
```


#### The third article uses a different html node and it needed to be scraped seperately 

```{r warning=FALSE, message= FALSE}
article1 <- url2 %>% map(read_html)
title <-
    article1 %>% map_chr(. %>% html_node("title") %>% html_text())
content <-
    article1 %>% map_chr(. %>% html_nodes('.main_content') %>% html_text() %>% paste(., collapse = ""))

article_table1 <- data.frame( "Title" = title, "Text" = content)
glimpse(article_table1)
```



#### Here we merge all three of the articles 
```{r warning=FALSE, message= FALSE}
article_table1_2<-merge(article_table,article_table1,all=TRUE)
glimpse(article_table1_2)
```



### Create a csv of all the articles

- Since all of the tables are merged we can now create a csv
```{r warning=FALSE, message= FALSE}
write.csv(article_table1_2,"social_media_and_adolescents.csv",row.names = TRUE)

```


### Load the csv into github and read it into R
```{r warning=FALSE, message= FALSE}
social_media_adolescents<-read.csv("https://raw.githubusercontent.com/Luz917/Data_607_Final-_Project/master/social_media_and_adolescents.csv", stringsAsFactors = FALSE) 

```

```{r warning=FALSE, message= FALSE}
str(social_media_adolescents)
```


## Clean up the data and prepare it for text analysation

### The data gets cleaned by removing stop words, including any additional words that one may not want to be included. Removing any words that are under three letters, and tokenizing the words so that each word has its own row. 

```{r warning=FALSE, message= FALSE}
## This method was found in datacamp with their example of sentiment analysis

undesirable_words <- c("thousand", "the", "hundred","three","thousand","percent","four", "null")


social_media_tidy <- social_media_adolescents %>%
  unnest_tokens(word, Text) %>%
  filter(!word %in% undesirable_words) %>% #Remove undesirables
  filter(!nchar(word) < 3) %>% #Words that are less than three letters 
  anti_join(stop_words) #Data provided by the tidytext package
glimpse(social_media_tidy)
```

## Text Analyzation : Sentiment Analysis


### First do a word count and see what were the most used words withn these three articles. 

```{r warning=FALSE, message= FALSE}
totalwords1<-social_media_tidy%>%
  count(word, sort= TRUE)

kable(totalwords1 [1:20, 1:2] ) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T, border_right = F) %>%
  column_spec(1, width = "15em", background = "lightblue")
```

- Here we can see that social media are the two most used words, after that it is online, then false, then health, a word that is a little bit alarming is depression. 


### Visualization of the word count 

```{r warning=FALSE, message= FALSE}
library(ggplot2)

social_media_tidy %>%
  count(word, sort = TRUE) %>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


## An observation of the three most used sentiments

### NRC Word-Emotion Association Lexicon (aka EmoLex)

- NRC is a list of Enlish words which associates with eight basic words: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. Along with two sentiments: positive and negative. 


```{r warning=FALSE, message= FALSE}
get_sentiments("nrc")

```


### Bing

- Evaluates the words as either positive or negative 

```{r warning=FALSE, message= FALSE}
get_sentiments("bing")
```

### Afinn

- Words in Afinn are given numerical values that are either positive or negative. 

```{r warning=FALSE, message= FALSE}
get_sentiments("afinn")
```

## Sentiment Analysis: NRC 

### Positive

- Now we begin the analysis, first I am going to use the NRC emotion "trust" which is considered positive and see what words relate to that to "trust". 

```{r warning=FALSE, message= FALSE}
nrc_trust <- get_sentiments("nrc") %>% 
  filter(sentiment == "trust")

social_media_nrc<-social_media_tidy  %>%
 
  inner_join(nrc_trust) %>%
  count(word, sort = TRUE)

kable(social_media_nrc ) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T, border_right = F) %>%
  column_spec(1, width = "15em", background = "lightblue")
```

- here we can see that the top five words are SHARE, COMMUNICATION, FOUND, PERSONAL, CENTER. 


### Negative

- Here I decided to use the NRC emotion word "sadness" to see what words that are considered negative.  

```{r warning=FALSE, message= FALSE}
nrc_sadness <- get_sentiments("nrc") %>% 
  filter(sentiment == "sadness")

social_media_nrcneg<-social_media_tidy%>%
  inner_join(nrc_sadness) %>%
  count(word, sort = TRUE)


kable(social_media_nrcneg ) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T, border_right = F) %>%
  column_spec(1, width = "15em", background = "lightblue")
```

- The very first word is DEPRESSION, followed by NEGATIVE, ANXIETY, SUICIDE, ILLNESS, and ISOLATION

## BING 

- Next we take a look at the sentiment BING

```{r warning=FALSE, message= FALSE}
bing_word_counts <- social_media_tidy %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

kable(bing_word_counts) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T, border_right = F) %>%
  column_spec(1, width = "15em", background = "lightblue")

```

- Here we can see that the most used words are mostly negative with words like FALSE, DEPRESSION, NEGATIVE, ANXIETY, SUICIDE



### Visualization of BING

```{r warning=FALSE, message= FALSE}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

```{r warning=FALSE, message= FALSE}
social_media_sentiment_bing <- social_media_tidy  %>%
  inner_join(get_sentiments("bing")) %>%
  count(Article_id, index = row_number() %/% 20, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

- Takes a closer look within each article and these articles contain mostly negative words 
```{r warning=FALSE, message= FALSE}
ggplot(social_media_sentiment_bing, aes(index, sentiment, fill = Article_id)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Article_id, ncol = 1, scales = "free_x")
```

## Afinn 

- Here we can see the average sentiment values of each of the articles. It is no surprise that the secon article has the most since it is about the negative effects of social media. 

```{r warning=FALSE, message= FALSE}

social_media_afinn <- social_media_tidy%>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(Article_id)%>%
  summarize(value = sum(value * n() / sum(n())))

social_media_afinn %>%
  mutate(Article_id = reorder(Article_id, value)) %>%
  ggplot(aes(Article_id, value, fill = value > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  ylab("Average sentiment value")
```



## Visualization of the three sentiments AFINN, Bing, and NRC

```{r warning=FALSE, message= FALSE}
afinn <- social_media_tidy %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = row_number() %/% 60) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(social_media_tidy %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing"),
                          social_media_tidy %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = row_number() %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```


```{r warning=FALSE, message= FALSE}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```


## Further analysis : Pairwise and Correlation 

### Pairwise

```{r warning=FALSE, message= FALSE}
social_media_pairs<-social_media_tidy %>% 
  pairwise_count(word, Article_id, sort = TRUE, upper = FALSE)

social_media_pairs
```
- Here we can take a look at the worded pairs.


### Visualization of pairwise 

```{r warning=FALSE, message= FALSE}
set.seed(1234)
social_media_pairs %>%
  filter(n >= 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "royalblue") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```



### Correlation of the words 

```{r warning=FALSE, message= FALSE}
social_media_cors <- social_media_tidy  %>% 
  group_by(word) %>%
  filter(n() >= 10) %>%
  pairwise_cor(word, Title, sort = TRUE, upper = FALSE)

social_media_cors
```


```{r warning=FALSE, message= FALSE}

set.seed(1234)
social_media_cors %>%
  filter(correlation > -.5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "lightblue") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.3, "lines")) +
  theme_void()
```
- You can see that some of the words have a negative correlation which are the lines that are faded 


## WORDCLOUDS

### With wordcloud one can visually see the words themselves

```{r warning=FALSE, message= FALSE}
social_media_tidy %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 15))
```


### These two wordclouds use the sentiment Bing

```{r warning=FALSE, message= FALSE}
social_media_tidy  %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray80", "gray20"),
                   max.words = 50)
```


```{r}
social_media_tidy  %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 50)
```

## Conclusion

Is social media good or bad for children and adolescnets? There seems to be some benefits to social media but is the good outweighed by the bad. Looking at the Wordclouds you can see some of the positives which are healthy, safe, smart, popular. But when looking at the negative words like depression, negative, false, suicide, risk, anxiety, illness, isolation, and unhealthy. All these negative words are very serious and worrisome. The three words that stick out the most to me are DEPRESSION, SUICIDE, and ANXIETY, these words are words that I would generally put when describing adolescents and children. It's a little scary that that is something that we have to worry about. Doing this project I am a bit more worried about my niece using social media, but its just something that we have to pay attention too.   


## Difficulties
There many difficulties doing this project, more so when cleaning the text. I wanted to use a different way to clean it but then realized that is was not the best method until I found the article in Datacamp that showed the best way to do it. 

## References 

1. NRC Word-Emotion Association Lexicon
http://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

2. Text Mining with R A Tidy Approach by Julia Silge and David Robinson
https://www.tidytextmining.com/index.html

3. Tidy Sentiment Analysis in R
https://www.datacamp.com/community/tutorials/sentiment-analysis-R

